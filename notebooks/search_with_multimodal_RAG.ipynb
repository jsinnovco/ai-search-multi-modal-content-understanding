{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Retrieval Augmented Generation with Content Understanding\n",
    "\n",
    "# Overview\n",
    "\n",
    "Azure AI Content Understanding provides a powerful solution for extracting data from diverse content types, while preserving semantic integrity and contextual relationships ensuring optimal performance in Retrieval Augmented Generation (RAG) applications.\n",
    "\n",
    "This sample demonstrates how to leverage Azure AI's Content Understanding capabilities to extract:\n",
    "\n",
    "- OCR and layout information from documents\n",
    "- Image description, summarization and classification\n",
    "- Audio transcription with speaker diarization from audio files\n",
    "- Shot detection, keyframe extraction, and audio transcription from videos\n",
    "\n",
    "This notebook illustrates how to extract content from unstructured multimodal data and apply it to Retrieval Augmented Generation (RAG). The resulting output can be converted to vector embeddings and indexed in Azure AI Search. When a user submits a query, Azure AI Search retrieves relevant chunks to generate a context-aware response.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-requisites\n",
    "1. Follow [README](../README.md#configure-azure-ai-service-resource) to create essential resource that will be used in this sample\n",
    "2. Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: PyMuPDF in /home/vscode/.local/lib/python3.11/site-packages (from -r ../requirements.txt (line 1)) (1.26.5)\n",
      "Requirement already satisfied: azure-ai-documentintelligence in /home/vscode/.local/lib/python3.11/site-packages (from -r ../requirements.txt (line 2)) (1.0.2)\n",
      "Requirement already satisfied: azure-core in /home/vscode/.local/lib/python3.11/site-packages (from -r ../requirements.txt (line 3)) (1.36.0)\n",
      "Requirement already satisfied: azure-identity in /home/vscode/.local/lib/python3.11/site-packages (from -r ../requirements.txt (line 4)) (1.25.1)\n",
      "Requirement already satisfied: azure-search-documents in /home/vscode/.local/lib/python3.11/site-packages (from -r ../requirements.txt (line 5)) (11.6.0b3)\n",
      "Requirement already satisfied: cffi in /home/vscode/.local/lib/python3.11/site-packages (from -r ../requirements.txt (line 6)) (2.0.0)\n",
      "Requirement already satisfied: langchain in /home/vscode/.local/lib/python3.11/site-packages (from -r ../requirements.txt (line 7)) (1.0.1)\n",
      "Requirement already satisfied: tiktoken in /home/vscode/.local/lib/python3.11/site-packages (from -r ../requirements.txt (line 8)) (0.12.0)\n",
      "Requirement already satisfied: python-dotenv in /home/vscode/.local/lib/python3.11/site-packages (from -r ../requirements.txt (line 9)) (1.1.1)\n",
      "Requirement already satisfied: pillow in /home/vscode/.local/lib/python3.11/site-packages (from -r ../requirements.txt (line 10)) (12.0.0)\n",
      "Requirement already satisfied: requests in /home/vscode/.local/lib/python3.11/site-packages (from -r ../requirements.txt (line 11)) (2.32.5)\n",
      "Requirement already satisfied: langchain-openai in /home/vscode/.local/lib/python3.11/site-packages (from -r ../requirements.txt (line 12)) (1.0.0)\n",
      "Requirement already satisfied: langchain-community in /home/vscode/.local/lib/python3.11/site-packages (from -r ../requirements.txt (line 13)) (0.4)\n",
      "Requirement already satisfied: langchain-core in /home/vscode/.local/lib/python3.11/site-packages (from -r ../requirements.txt (line 14)) (1.0.0)\n",
      "Requirement already satisfied: langchain-text-splitters in /home/vscode/.local/lib/python3.11/site-packages (from -r ../requirements.txt (line 15)) (1.0.0)\n",
      "Requirement already satisfied: langchainhub in /home/vscode/.local/lib/python3.11/site-packages (from -r ../requirements.txt (line 16)) (0.1.21)\n",
      "Requirement already satisfied: openai in /home/vscode/.local/lib/python3.11/site-packages (from -r ../requirements.txt (line 17)) (2.5.0)\n",
      "Requirement already satisfied: Flask==2.2.5 in /home/vscode/.local/lib/python3.11/site-packages (from -r ../requirements.txt (line 18)) (2.2.5)\n",
      "Requirement already satisfied: flask-cors==5.0.1 in /home/vscode/.local/lib/python3.11/site-packages (from -r ../requirements.txt (line 19)) (5.0.1)\n",
      "Requirement already satisfied: azure-storage-blob>=12.18.0 in /home/vscode/.local/lib/python3.11/site-packages (from -r ../requirements.txt (line 20)) (12.27.0)\n",
      "Requirement already satisfied: Werkzeug>=2.2.2 in /home/vscode/.local/lib/python3.11/site-packages (from Flask==2.2.5->-r ../requirements.txt (line 18)) (3.1.3)\n",
      "Requirement already satisfied: Jinja2>=3.0 in /home/vscode/.local/lib/python3.11/site-packages (from Flask==2.2.5->-r ../requirements.txt (line 18)) (3.1.6)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in /home/vscode/.local/lib/python3.11/site-packages (from Flask==2.2.5->-r ../requirements.txt (line 18)) (2.2.0)\n",
      "Requirement already satisfied: click>=8.0 in /home/vscode/.local/lib/python3.11/site-packages (from Flask==2.2.5->-r ../requirements.txt (line 18)) (8.3.0)\n",
      "Requirement already satisfied: isodate>=0.6.1 in /home/vscode/.local/lib/python3.11/site-packages (from azure-ai-documentintelligence->-r ../requirements.txt (line 2)) (0.7.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /home/vscode/.local/lib/python3.11/site-packages (from azure-ai-documentintelligence->-r ../requirements.txt (line 2)) (4.15.0)\n",
      "Requirement already satisfied: cryptography>=2.5 in /home/vscode/.local/lib/python3.11/site-packages (from azure-identity->-r ../requirements.txt (line 4)) (46.0.3)\n",
      "Requirement already satisfied: msal>=1.30.0 in /home/vscode/.local/lib/python3.11/site-packages (from azure-identity->-r ../requirements.txt (line 4)) (1.34.0)\n",
      "Requirement already satisfied: msal-extensions>=1.2.0 in /home/vscode/.local/lib/python3.11/site-packages (from azure-identity->-r ../requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: azure-common>=1.1 in /home/vscode/.local/lib/python3.11/site-packages (from azure-search-documents->-r ../requirements.txt (line 5)) (1.1.28)\n",
      "Requirement already satisfied: pycparser in /home/vscode/.local/lib/python3.11/site-packages (from cffi->-r ../requirements.txt (line 6)) (2.23)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.0 in /home/vscode/.local/lib/python3.11/site-packages (from langchain->-r ../requirements.txt (line 7)) (1.0.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /home/vscode/.local/lib/python3.11/site-packages (from langchain->-r ../requirements.txt (line 7)) (2.12.3)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /home/vscode/.local/lib/python3.11/site-packages (from langchain-core->-r ../requirements.txt (line 14)) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /home/vscode/.local/lib/python3.11/site-packages (from langchain-core->-r ../requirements.txt (line 14)) (0.4.37)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /home/vscode/.local/lib/python3.11/site-packages (from langchain-core->-r ../requirements.txt (line 14)) (24.2)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /home/vscode/.local/lib/python3.11/site-packages (from langchain-core->-r ../requirements.txt (line 14)) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /home/vscode/.local/lib/python3.11/site-packages (from langchain-core->-r ../requirements.txt (line 14)) (9.1.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/vscode/.local/lib/python3.11/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core->-r ../requirements.txt (line 14)) (3.0.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in /home/vscode/.local/lib/python3.11/site-packages (from langgraph<1.1.0,>=1.0.0->langchain->-r ../requirements.txt (line 7)) (2.1.2)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.0 in /home/vscode/.local/lib/python3.11/site-packages (from langgraph<1.1.0,>=1.0.0->langchain->-r ../requirements.txt (line 7)) (1.0.0)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /home/vscode/.local/lib/python3.11/site-packages (from langgraph<1.1.0,>=1.0.0->langchain->-r ../requirements.txt (line 7)) (0.2.9)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /home/vscode/.local/lib/python3.11/site-packages (from langgraph<1.1.0,>=1.0.0->langchain->-r ../requirements.txt (line 7)) (3.6.0)\n",
      "Requirement already satisfied: ormsgpack>=1.10.0 in /home/vscode/.local/lib/python3.11/site-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.0->langchain->-r ../requirements.txt (line 7)) (1.11.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /home/vscode/.local/lib/python3.11/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain->-r ../requirements.txt (line 7)) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /home/vscode/.local/lib/python3.11/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain->-r ../requirements.txt (line 7)) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /home/vscode/.local/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core->-r ../requirements.txt (line 14)) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /home/vscode/.local/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core->-r ../requirements.txt (line 14)) (0.25.0)\n",
      "Requirement already satisfied: anyio in /home/vscode/.local/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain->-r ../requirements.txt (line 7)) (4.11.0)\n",
      "Requirement already satisfied: certifi in /home/vscode/.local/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain->-r ../requirements.txt (line 7)) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /home/vscode/.local/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain->-r ../requirements.txt (line 7)) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/vscode/.local/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain->-r ../requirements.txt (line 7)) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /home/vscode/.local/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain->-r ../requirements.txt (line 7)) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/vscode/.local/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r ../requirements.txt (line 7)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /home/vscode/.local/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r ../requirements.txt (line 7)) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/vscode/.local/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r ../requirements.txt (line 7)) (0.4.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/vscode/.local/lib/python3.11/site-packages (from tiktoken->-r ../requirements.txt (line 8)) (2025.9.18)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/vscode/.local/lib/python3.11/site-packages (from requests->-r ../requirements.txt (line 11)) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vscode/.local/lib/python3.11/site-packages (from requests->-r ../requirements.txt (line 11)) (2.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/vscode/.local/lib/python3.11/site-packages (from openai->-r ../requirements.txt (line 17)) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /home/vscode/.local/lib/python3.11/site-packages (from openai->-r ../requirements.txt (line 17)) (0.11.1)\n",
      "Requirement already satisfied: sniffio in /home/vscode/.local/lib/python3.11/site-packages (from openai->-r ../requirements.txt (line 17)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/vscode/.local/lib/python3.11/site-packages (from openai->-r ../requirements.txt (line 17)) (4.67.1)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /home/vscode/.local/lib/python3.11/site-packages (from langchain-community->-r ../requirements.txt (line 13)) (1.0.0)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /home/vscode/.local/lib/python3.11/site-packages (from langchain-community->-r ../requirements.txt (line 13)) (2.0.44)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/vscode/.local/lib/python3.11/site-packages (from langchain-community->-r ../requirements.txt (line 13)) (3.13.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /home/vscode/.local/lib/python3.11/site-packages (from langchain-community->-r ../requirements.txt (line 13)) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /home/vscode/.local/lib/python3.11/site-packages (from langchain-community->-r ../requirements.txt (line 13)) (2.11.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /home/vscode/.local/lib/python3.11/site-packages (from langchain-community->-r ../requirements.txt (line 13)) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /home/vscode/.local/lib/python3.11/site-packages (from langchain-community->-r ../requirements.txt (line 13)) (2.3.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/vscode/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r ../requirements.txt (line 13)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/vscode/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r ../requirements.txt (line 13)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/vscode/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r ../requirements.txt (line 13)) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/vscode/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r ../requirements.txt (line 13)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/vscode/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r ../requirements.txt (line 13)) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/vscode/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r ../requirements.txt (line 13)) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/vscode/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r ../requirements.txt (line 13)) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/vscode/.local/lib/python3.11/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community->-r ../requirements.txt (line 13)) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/vscode/.local/lib/python3.11/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community->-r ../requirements.txt (line 13)) (0.9.0)\n",
      "Requirement already satisfied: greenlet>=1 in /home/vscode/.local/lib/python3.11/site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community->-r ../requirements.txt (line 13)) (3.2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/vscode/.local/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community->-r ../requirements.txt (line 13)) (1.1.0)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /home/vscode/.local/lib/python3.11/site-packages (from langchainhub->-r ../requirements.txt (line 16)) (2.32.4.20250913)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/vscode/.local/lib/python3.11/site-packages (from Jinja2>=3.0->Flask==2.2.5->-r ../requirements.txt (line 18)) (3.0.3)\n",
      "Requirement already satisfied: PyJWT<3,>=1.0.0 in /home/vscode/.local/lib/python3.11/site-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity->-r ../requirements.txt (line 4)) (2.10.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Load and validate Azure AI Services configs\n",
    "AZURE_AI_SERVICE_ENDPOINT = os.getenv(\"AZURE_AI_SERVICE_ENDPOINT\")\n",
    "AZURE_AI_SERVICE_API_VERSION = os.getenv(\"AZURE_AI_SERVICE_API_VERSION\") or \"2024-12-01-preview\"\n",
    "AZURE_DOCUMENT_INTELLIGENCE_API_VERSION = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_API_VERSION\") or \"2024-11-30\"\n",
    "\n",
    "# Load and validate Azure OpenAI configs\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_CHAT_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "AZURE_OPENAI_CHAT_API_VERSION = os.getenv(\"AZURE_OPENAI_CHAT_API_VERSION\") or \"2024-08-01-preview\"\n",
    "AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "AZURE_OPENAI_EMBEDDING_API_VERSION = os.getenv(\"AZURE_OPENAI_EMBEDDING_API_VERSION\") or \"2023-05-15\"\n",
    "\n",
    "# Load and validate Azure Search Services configs\n",
    "AZURE_SEARCH_ENDPOINT = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "AZURE_SEARCH_INDEX_NAME = os.getenv(\"AZURE_SEARCH_INDEX_NAME\") or \"sample-doc-index\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create custom analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic import hub\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables.passthrough import RunnablePassthrough\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "from langchain_community.vectorstores.azuresearch import AzureSearch\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents.base import Document\n",
    "import requests\n",
    "import json\n",
    "import sys\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "# Add the parent directory to the path to use shared modules\n",
    "parent_dir = Path(Path.cwd()).parent\n",
    "sys.path.append(str(parent_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create analyzers with pre-defined schemas.\n",
    "Feel free to start with the provided sample data as a reference and experiment with your own data to explore its capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created analyzer: doc-analyzere998a576-5e12-4546-a1c7-762705268633\n",
      "Successfully created analyzer: image-analyzer6bc0fdba-9809-4c0b-accd-43b7a1f1012b\n",
      "Successfully created analyzer: audio-analyzer37f08faa-5398-4edf-ad33-fbc12e83bcb2\n",
      "Successfully created analyzer: video-analyzer2b72a252-b817-4bc5-b7dd-e2142f6fed54\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from python.content_understanding_client import AzureContentUnderstandingClient\n",
    "credential = DefaultAzureCredential()\n",
    "token_provider = get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "#set analyzer configs\n",
    "analyzer_configs = [\n",
    "    {\n",
    "        \"id\": \"doc-analyzer\" + str(uuid.uuid4()),\n",
    "        \"template_path\": \"../analyzer_templates/content_document.json\",\n",
    "        \"location\": Path(\"../data/credit_card_application_JessicaSmith.pdf\"),\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"image-analyzer\" + str(uuid.uuid4()),\n",
    "        \"template_path\": \"../analyzer_templates/image_chart_diagram_understanding.json\",\n",
    "        \"location\": Path(\"../data/SSN_JessicaSmith.png\"),\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"audio-analyzer\" + str(uuid.uuid4()),\n",
    "        \"template_path\": \"../analyzer_templates/call_recording_analytics.json\",\n",
    "        \"location\": Path(\"../data/callCenterRecording.mp3\"),\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"video-analyzer\" + str(uuid.uuid4()),\n",
    "        \"template_path\": \"../analyzer_templates/video_content_understanding.json\",\n",
    "        \"location\": Path(\"../data/FlightSimulator.mp4\"),\n",
    "    },\n",
    "]\n",
    "\n",
    "# Create Content Understanding client\n",
    "content_understanding_client = AzureContentUnderstandingClient(\n",
    "    endpoint=AZURE_AI_SERVICE_ENDPOINT,\n",
    "    api_version=AZURE_AI_SERVICE_API_VERSION,\n",
    "    token_provider=token_provider,\n",
    "    x_ms_useragent=\"azure-ai-content-understanding-python/content_extraction\", # This header is used for sample usage telemetry, please comment out this line if you want to opt out.\n",
    ")\n",
    "\n",
    "# Iterate through each config and create an analyzer\n",
    "for analyzer in analyzer_configs:\n",
    "    analyzer_id = analyzer[\"id\"]\n",
    "    template_path = analyzer[\"template_path\"]\n",
    "\n",
    "    try:\n",
    "        \n",
    "        # Create the analyzer using the content understanding client\n",
    "        response = content_understanding_client.begin_create_analyzer(\n",
    "            analyzer_id=analyzer_id,\n",
    "            analyzer_template_path=template_path\n",
    "        )\n",
    "        result = content_understanding_client.poll_result(response)\n",
    "        print(f\"Successfully created analyzer: {analyzer_id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create analyzer: {analyzer_id}\")\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use created analyzers to extract multimodal content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argument of type 'PosixPath' is not iterable\n",
      "Error in creating analyzer. Please double-check your analysis settings.\n",
      "If there is a conflict, you can delete the analyzer and then recreate it, or move to the next cell and use the existing analyzer.\n",
      "argument of type 'PosixPath' is not iterable\n",
      "Error in creating analyzer. Please double-check your analysis settings.\n",
      "If there is a conflict, you can delete the analyzer and then recreate it, or move to the next cell and use the existing analyzer.\n",
      "Analyzer Results:\n",
      "Analyzer ID: audio-analyzer37f08faa-5398-4edf-ad33-fbc12e83bcb2\n",
      "{\n",
      "  \"analyzerId\": \"audio-analyzer37f08faa-5398-4edf-ad33-fbc12e83bcb2\",\n",
      "  \"apiVersion\": \"2024-12-01-preview\",\n",
      "  \"createdAt\": \"2026-01-15T21:21:25Z\",\n",
      "  \"warnings\": [],\n",
      "  \"contents\": [\n",
      "    {\n",
      "      \"markdown\": \"```WEBVTT\\n\\n00:00.080 --> 00:00.640\\n<v Agent>Good day.\\n\\n00:00.880 --> 00:02.240\\n<v Agent>Welcome to Contoso.\\n\\n00:02.560 --> 00:03.760\\n<v Agent>My name is John Doe.\\n\\n00:03.920 --> 00:05.120\\n<v Agent>How can I help you today?\\n\\n00:05.440 --> 00:06.320\\n<v Customer>Yes, good day.\\n\\n00:06.640 --> 00:08.160\\n<v Customer>My name is Maria Smith.\\n\\n00:08.560 --> 00:11.360\\n<v Customer>I would like to inquire about my current point balance.\\n\\n00:11.680 --> 00:12.560\\n<v Agent>No problem.\\n\\n00:12.880 --> 00:13.920\\n<v Agent>I am happy to help.\\n\\n00:14.240 --> 00:16.720\\n<v Agent>I need your date of birth to confirm your identity.\\n\\n00:17.120 --> 00:19.600\\n<v Customer>It is April 19th, 1988.\\n\\n00:20.000 --> 00:20.480\\n<v Agent>Great.\\n\\n00:20.800 --> 00:24.160\\n<v Agent>Your current point balance is 599 points.\\n\\n00:24.560 --> 00:26.160\\n<v Agent>Do you need any more information?\\n\\n00:26.480 --> 00:27.200\\n<v Customer>No, thank you.\\n\\n00:27.600 --> 00:28.320\\n<v Customer>That was all.\\n\\n00:28.720 --> 00:29.360\\n<v Customer>Goodbye.\\n\\n00:29.680 --> 00:31.920\\n<v Agent>You're welcome, goodbye a Cantoso.```\",\n",
      "      \"fields\": {\n",
      "        \"Sentiment\": {\n",
      "          \"type\": \"string\",\n",
      "          \"valueString\": \"Positive\"\n",
      "        },\n",
      "        \"Summary\": {\n",
      "          \"type\": \"string\",\n",
      "          \"valueString\": \"Maria Smith contacted Contoso to inquire about her current point balance. Agent John Doe assisted her by confirming her identity and providing the information that her balance is 599 points. The interaction was brief and positive, with Maria expressing satisfaction with the information provided.\"\n",
      "        },\n",
      "        \"Categories\": {\n",
      "          \"type\": \"array\",\n",
      "          \"valueArray\": [\n",
      "            {\n",
      "              \"type\": \"string\",\n",
      "              \"valueString\": \"Retail\"\n",
      "            }\n",
      "          ]\n",
      "        },\n",
      "        \"Companies\": {\n",
      "          \"type\": \"array\",\n",
      "          \"valueArray\": [\n",
      "            {\n",
      "              \"type\": \"string\",\n",
      "              \"valueString\": \"Contoso\"\n",
      "            }\n",
      "          ]\n",
      "        },\n",
      "        \"People\": {\n",
      "          \"type\": \"array\",\n",
      "          \"valueArray\": [\n",
      "            {\n",
      "              \"type\": \"object\",\n",
      "              \"valueObject\": {\n",
      "                \"Name\": {\n",
      "                  \"type\": \"string\",\n",
      "                  \"valueString\": \"John Doe\"\n",
      "                },\n",
      "                \"Role\": {\n",
      "                  \"type\": \"string\",\n",
      "                  \"valueString\": \"Agent\"\n",
      "                }\n",
      "              }\n",
      "            },\n",
      "            {\n",
      "              \"type\": \"object\",\n",
      "              \"valueObject\": {\n",
      "                \"Name\": {\n",
      "                  \"type\": \"string\",\n",
      "                  \"valueString\": \"Maria Smith\"\n",
      "                },\n",
      "                \"Role\": {\n",
      "                  \"type\": \"string\",\n",
      "                  \"valueString\": \"Customer\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "          ]\n",
      "        },\n",
      "        \"Topics\": {\n",
      "          \"type\": \"array\",\n",
      "          \"valueArray\": [\n",
      "            {\n",
      "              \"type\": \"string\",\n",
      "              \"valueString\": \"Point balance inquiry\"\n",
      "            },\n",
      "            {\n",
      "              \"type\": \"string\",\n",
      "              \"valueString\": \"Customer service\"\n",
      "            },\n",
      "            {\n",
      "              \"type\": \"string\",\n",
      "              \"valueString\": \"Identity confirmation\"\n",
      "            },\n",
      "            {\n",
      "              \"type\": \"string\",\n",
      "              \"valueString\": \"Account information\"\n",
      "            },\n",
      "            {\n",
      "              \"type\": \"string\",\n",
      "              \"valueString\": \"Retail points\"\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      },\n",
      "      \"kind\": \"audioVisual\",\n",
      "      \"startTimeMs\": 0,\n",
      "      \"endTimeMs\": 32182,\n",
      "      \"transcriptPhrases\": [\n",
      "        {\n",
      "          \"speaker\": \"Agent\",\n",
      "          \"startTimeMs\": 80,\n",
      "          \"endTimeMs\": 640,\n",
      "          \"text\": \"Good day.\",\n",
      "          \"confidence\": 0.926,\n",
      "          \"words\": [\n",
      "            {\n",
      "              \"startTimeMs\": 80,\n",
      "              \"endTimeMs\": 280,\n",
      "              \"text\": \"Good\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 280,\n",
      "              \"endTimeMs\": 640,\n",
      "              \"text\": \"day.\"\n",
      "            }\n",
      "          ],\n",
      "          \"locale\": \"en-US\"\n",
      "        },\n",
      "        {\n",
      "          \"speaker\": \"Agent\",\n",
      "          \"startTimeMs\": 880,\n",
      "          \"endTimeMs\": 2240,\n",
      "          \"text\": \"Welcome to Contoso.\",\n",
      "          \"confidence\": 0.926,\n",
      "          \"words\": [\n",
      "            {\n",
      "              \"startTimeMs\": 880,\n",
      "              \"endTimeMs\": 1360,\n",
      "              \"text\": \"Welcome\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 1360,\n",
      "              \"endTimeMs\": 1480,\n",
      "              \"text\": \"to\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 1480,\n",
      "              \"endTimeMs\": 2240,\n",
      "              \"text\": \"Contoso.\"\n",
      "            }\n",
      "          ],\n",
      "          \"locale\": \"en-US\"\n",
      "        },\n",
      "        {\n",
      "          \"speaker\": \"Agent\",\n",
      "          \"startTimeMs\": 2560,\n",
      "          \"endTimeMs\": 3760,\n",
      "          \"text\": \"My name is John Doe.\",\n",
      "          \"confidence\": 0.926,\n",
      "          \"words\": [\n",
      "            {\n",
      "              \"startTimeMs\": 2560,\n",
      "              \"endTimeMs\": 2720,\n",
      "              \"text\": \"My\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 2720,\n",
      "              \"endTimeMs\": 2960,\n",
      "              \"text\": \"name\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 2960,\n",
      "              \"endTimeMs\": 3120,\n",
      "              \"text\": \"is\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 3120,\n",
      "              \"endTimeMs\": 3440,\n",
      "              \"text\": \"John\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 3440,\n",
      "              \"endTimeMs\": 3760,\n",
      "              \"text\": \"Doe.\"\n",
      "            }\n",
      "          ],\n",
      "          \"locale\": \"en-US\"\n",
      "        },\n",
      "        {\n",
      "          \"speaker\": \"Agent\",\n",
      "          \"startTimeMs\": 3920,\n",
      "          \"endTimeMs\": 5120,\n",
      "          \"text\": \"How can I help you today?\",\n",
      "          \"confidence\": 0.926,\n",
      "          \"words\": [\n",
      "            {\n",
      "              \"startTimeMs\": 3920,\n",
      "              \"endTimeMs\": 4120,\n",
      "              \"text\": \"How\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 4120,\n",
      "              \"endTimeMs\": 4240,\n",
      "              \"text\": \"can\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 4240,\n",
      "              \"endTimeMs\": 4320,\n",
      "              \"text\": \"I\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 4320,\n",
      "              \"endTimeMs\": 4640,\n",
      "              \"text\": \"help\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 4640,\n",
      "              \"endTimeMs\": 4720,\n",
      "              \"text\": \"you\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 4720,\n",
      "              \"endTimeMs\": 5120,\n",
      "              \"text\": \"today?\"\n",
      "            }\n",
      "          ],\n",
      "          \"locale\": \"en-US\"\n",
      "        },\n",
      "        {\n",
      "          \"speaker\": \"Customer\",\n",
      "          \"startTimeMs\": 5440,\n",
      "          \"endTimeMs\": 6320,\n",
      "          \"text\": \"Yes, good day.\",\n",
      "          \"confidence\": 0.926,\n",
      "          \"words\": [\n",
      "            {\n",
      "              \"startTimeMs\": 5440,\n",
      "              \"endTimeMs\": 5760,\n",
      "              \"text\": \"Yes,\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 5760,\n",
      "              \"endTimeMs\": 5960,\n",
      "              \"text\": \"good\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 5960,\n",
      "              \"endTimeMs\": 6320,\n",
      "              \"text\": \"day.\"\n",
      "            }\n",
      "          ],\n",
      "          \"locale\": \"en-US\"\n",
      "        },\n",
      "        {\n",
      "          \"speaker\": \"Customer\",\n",
      "          \"startTimeMs\": 6640,\n",
      "          \"endTimeMs\": 8160,\n",
      "          \"text\": \"My name is Maria Smith.\",\n",
      "          \"confidence\": 0.926,\n",
      "          \"words\": [\n",
      "            {\n",
      "              \"startTimeMs\": 6640,\n",
      "              \"endTimeMs\": 6880,\n",
      "              \"text\": \"My\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 6880,\n",
      "              \"endTimeMs\": 7120,\n",
      "              \"text\": \"name\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 7120,\n",
      "              \"endTimeMs\": 7280,\n",
      "              \"text\": \"is\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 7280,\n",
      "              \"endTimeMs\": 7680,\n",
      "              \"text\": \"Maria\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 7680,\n",
      "              \"endTimeMs\": 8160,\n",
      "              \"text\": \"Smith.\"\n",
      "            }\n",
      "          ],\n",
      "          \"locale\": \"en-US\"\n",
      "        },\n",
      "        {\n",
      "          \"speaker\": \"Customer\",\n",
      "          \"startTimeMs\": 8560,\n",
      "          \"endTimeMs\": 11360,\n",
      "          \"text\": \"I would like to inquire about my current point balance.\",\n",
      "          \"confidence\": 0.926,\n",
      "          \"words\": [\n",
      "            {\n",
      "              \"startTimeMs\": 8560,\n",
      "              \"endTimeMs\": 8640,\n",
      "              \"text\": \"I\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 8640,\n",
      "              \"endTimeMs\": 8800,\n",
      "              \"text\": \"would\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 8800,\n",
      "              \"endTimeMs\": 9040,\n",
      "              \"text\": \"like\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 9040,\n",
      "              \"endTimeMs\": 9200,\n",
      "              \"text\": \"to\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 9200,\n",
      "              \"endTimeMs\": 9720,\n",
      "              \"text\": \"inquire\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 9720,\n",
      "              \"endTimeMs\": 9920,\n",
      "              \"text\": \"about\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 9920,\n",
      "              \"endTimeMs\": 10080,\n",
      "              \"text\": \"my\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 10080,\n",
      "              \"endTimeMs\": 10440,\n",
      "              \"text\": \"current\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 10440,\n",
      "              \"endTimeMs\": 10720,\n",
      "              \"text\": \"point\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 10720,\n",
      "              \"endTimeMs\": 11360,\n",
      "              \"text\": \"balance.\"\n",
      "            }\n",
      "          ],\n",
      "          \"locale\": \"en-US\"\n",
      "        },\n",
      "        {\n",
      "          \"speaker\": \"Agent\",\n",
      "          \"startTimeMs\": 11680,\n",
      "          \"endTimeMs\": 12560,\n",
      "          \"text\": \"No problem.\",\n",
      "          \"confidence\": 0.926,\n",
      "          \"words\": [\n",
      "            {\n",
      "              \"startTimeMs\": 11680,\n",
      "              \"endTimeMs\": 11920,\n",
      "              \"text\": \"No\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 11920,\n",
      "              \"endTimeMs\": 12560,\n",
      "              \"text\": \"problem.\"\n",
      "            }\n",
      "          ],\n",
      "          \"locale\": \"en-US\"\n",
      "        },\n",
      "        {\n",
      "          \"speaker\": \"Agent\",\n",
      "          \"startTimeMs\": 12880,\n",
      "          \"endTimeMs\": 13920,\n",
      "          \"text\": \"I am happy to help.\",\n",
      "          \"confidence\": 0.926,\n",
      "          \"words\": [\n",
      "            {\n",
      "              \"startTimeMs\": 12880,\n",
      "              \"endTimeMs\": 13000,\n",
      "              \"text\": \"I\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 13000,\n",
      "              \"endTimeMs\": 13120,\n",
      "              \"text\": \"am\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 13120,\n",
      "              \"endTimeMs\": 13480,\n",
      "              \"text\": \"happy\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 13480,\n",
      "              \"endTimeMs\": 13560,\n",
      "              \"text\": \"to\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 13560,\n",
      "              \"endTimeMs\": 13920,\n",
      "              \"text\": \"help.\"\n",
      "            }\n",
      "          ],\n",
      "          \"locale\": \"en-US\"\n",
      "        },\n",
      "        {\n",
      "          \"speaker\": \"Agent\",\n",
      "          \"startTimeMs\": 14240,\n",
      "          \"endTimeMs\": 16720,\n",
      "          \"text\": \"I need your date of birth to confirm your identity.\",\n",
      "          \"confidence\": 0.926,\n",
      "          \"words\": [\n",
      "            {\n",
      "              \"startTimeMs\": 14240,\n",
      "              \"endTimeMs\": 14400,\n",
      "              \"text\": \"I\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 14400,\n",
      "              \"endTimeMs\": 14600,\n",
      "              \"text\": \"need\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 14600,\n",
      "              \"endTimeMs\": 14720,\n",
      "              \"text\": \"your\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 14720,\n",
      "              \"endTimeMs\": 14920,\n",
      "              \"text\": \"date\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 14920,\n",
      "              \"endTimeMs\": 15040,\n",
      "              \"text\": \"of\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 15040,\n",
      "              \"endTimeMs\": 15440,\n",
      "              \"text\": \"birth\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 15440,\n",
      "              \"endTimeMs\": 15560,\n",
      "              \"text\": \"to\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 15560,\n",
      "              \"endTimeMs\": 16000,\n",
      "              \"text\": \"confirm\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 16000,\n",
      "              \"endTimeMs\": 16160,\n",
      "              \"text\": \"your\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 16160,\n",
      "              \"endTimeMs\": 16720,\n",
      "              \"text\": \"identity.\"\n",
      "            }\n",
      "          ],\n",
      "          \"locale\": \"en-US\"\n",
      "        },\n",
      "        {\n",
      "          \"speaker\": \"Customer\",\n",
      "          \"startTimeMs\": 17120,\n",
      "          \"endTimeMs\": 19600,\n",
      "          \"text\": \"It is April 19th, 1988.\",\n",
      "          \"confidence\": 0.926,\n",
      "          \"words\": [\n",
      "            {\n",
      "              \"startTimeMs\": 17120,\n",
      "              \"endTimeMs\": 17280,\n",
      "              \"text\": \"It\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 17280,\n",
      "              \"endTimeMs\": 17440,\n",
      "              \"text\": \"is\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 17440,\n",
      "              \"endTimeMs\": 17760,\n",
      "              \"text\": \"April\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 17760,\n",
      "              \"endTimeMs\": 18480,\n",
      "              \"text\": \"19th,\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 18480,\n",
      "              \"endTimeMs\": 19600,\n",
      "              \"text\": \"1988.\"\n",
      "            }\n",
      "          ],\n",
      "          \"locale\": \"en-US\"\n",
      "        },\n",
      "        {\n",
      "          \"speaker\": \"Agent\",\n",
      "          \"startTimeMs\": 20000,\n",
      "          \"endTimeMs\": 20480,\n",
      "          \"text\": \"Great.\",\n",
      "          \"confidence\": 0.926,\n",
      "          \"words\": [\n",
      "            {\n",
      "              \"startTimeMs\": 20000,\n",
      "              \"endTimeMs\": 20480,\n",
      "              \"text\": \"Great.\"\n",
      "            }\n",
      "          ],\n",
      "          \"locale\": \"en-US\"\n",
      "        },\n",
      "        {\n",
      "          \"speaker\": \"Agent\",\n",
      "          \"startTimeMs\": 20800,\n",
      "          \"endTimeMs\": 24160,\n",
      "          \"text\": \"Your current point balance is 599 points.\",\n",
      "          \"confidence\": 0.926,\n",
      "          \"words\": [\n",
      "            {\n",
      "              \"startTimeMs\": 20800,\n",
      "              \"endTimeMs\": 21040,\n",
      "              \"text\": \"Your\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 21040,\n",
      "              \"endTimeMs\": 21360,\n",
      "              \"text\": \"current\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 21360,\n",
      "              \"endTimeMs\": 21600,\n",
      "              \"text\": \"point\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 21600,\n",
      "              \"endTimeMs\": 22160,\n",
      "              \"text\": \"balance\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 22160,\n",
      "              \"endTimeMs\": 22320,\n",
      "              \"text\": \"is\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 22320,\n",
      "              \"endTimeMs\": 23600,\n",
      "              \"text\": \"599\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 23600,\n",
      "              \"endTimeMs\": 24160,\n",
      "              \"text\": \"points.\"\n",
      "            }\n",
      "          ],\n",
      "          \"locale\": \"en-US\"\n",
      "        },\n",
      "        {\n",
      "          \"speaker\": \"Agent\",\n",
      "          \"startTimeMs\": 24560,\n",
      "          \"endTimeMs\": 26160,\n",
      "          \"text\": \"Do you need any more information?\",\n",
      "          \"confidence\": 0.926,\n",
      "          \"words\": [\n",
      "            {\n",
      "              \"startTimeMs\": 24560,\n",
      "              \"endTimeMs\": 24680,\n",
      "              \"text\": \"Do\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 24680,\n",
      "              \"endTimeMs\": 24800,\n",
      "              \"text\": \"you\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 24800,\n",
      "              \"endTimeMs\": 25040,\n",
      "              \"text\": \"need\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 25040,\n",
      "              \"endTimeMs\": 25200,\n",
      "              \"text\": \"any\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 25200,\n",
      "              \"endTimeMs\": 25440,\n",
      "              \"text\": \"more\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 25440,\n",
      "              \"endTimeMs\": 26160,\n",
      "              \"text\": \"information?\"\n",
      "            }\n",
      "          ],\n",
      "          \"locale\": \"en-US\"\n",
      "        },\n",
      "        {\n",
      "          \"speaker\": \"Customer\",\n",
      "          \"startTimeMs\": 26480,\n",
      "          \"endTimeMs\": 27200,\n",
      "          \"text\": \"No, thank you.\",\n",
      "          \"confidence\": 0.926,\n",
      "          \"words\": [\n",
      "            {\n",
      "              \"startTimeMs\": 26480,\n",
      "              \"endTimeMs\": 26640,\n",
      "              \"text\": \"No,\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 26640,\n",
      "              \"endTimeMs\": 26960,\n",
      "              \"text\": \"thank\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 26960,\n",
      "              \"endTimeMs\": 27200,\n",
      "              \"text\": \"you.\"\n",
      "            }\n",
      "          ],\n",
      "          \"locale\": \"en-US\"\n",
      "        },\n",
      "        {\n",
      "          \"speaker\": \"Customer\",\n",
      "          \"startTimeMs\": 27600,\n",
      "          \"endTimeMs\": 28320,\n",
      "          \"text\": \"That was all.\",\n",
      "          \"confidence\": 0.926,\n",
      "          \"words\": [\n",
      "            {\n",
      "              \"startTimeMs\": 27600,\n",
      "              \"endTimeMs\": 27800,\n",
      "              \"text\": \"That\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 27800,\n",
      "              \"endTimeMs\": 28000,\n",
      "              \"text\": \"was\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 28000,\n",
      "              \"endTimeMs\": 28320,\n",
      "              \"text\": \"all.\"\n",
      "            }\n",
      "          ],\n",
      "          \"locale\": \"en-US\"\n",
      "        },\n",
      "        {\n",
      "          \"speaker\": \"Customer\",\n",
      "          \"startTimeMs\": 28720,\n",
      "          \"endTimeMs\": 29360,\n",
      "          \"text\": \"Goodbye.\",\n",
      "          \"confidence\": 0.926,\n",
      "          \"words\": [\n",
      "            {\n",
      "              \"startTimeMs\": 28720,\n",
      "              \"endTimeMs\": 29360,\n",
      "              \"text\": \"Goodbye.\"\n",
      "            }\n",
      "          ],\n",
      "          \"locale\": \"en-US\"\n",
      "        },\n",
      "        {\n",
      "          \"speaker\": \"Agent\",\n",
      "          \"startTimeMs\": 29680,\n",
      "          \"endTimeMs\": 31920,\n",
      "          \"text\": \"You're welcome, goodbye a Cantoso.\",\n",
      "          \"confidence\": 0.413,\n",
      "          \"words\": [\n",
      "            {\n",
      "              \"startTimeMs\": 29680,\n",
      "              \"endTimeMs\": 29840,\n",
      "              \"text\": \"You're\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 29840,\n",
      "              \"endTimeMs\": 30400,\n",
      "              \"text\": \"welcome,\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 30640,\n",
      "              \"endTimeMs\": 31080,\n",
      "              \"text\": \"goodbye\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 31080,\n",
      "              \"endTimeMs\": 31120,\n",
      "              \"text\": \"a\"\n",
      "            },\n",
      "            {\n",
      "              \"startTimeMs\": 31120,\n",
      "              \"endTimeMs\": 31920,\n",
      "              \"text\": \"Cantoso.\"\n",
      "            }\n",
      "          ],\n",
      "          \"locale\": \"en-US\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Analyzer ID: video-analyzer2b72a252-b817-4bc5-b7dd-e2142f6fed54\n",
      "{\n",
      "  \"analyzerId\": \"video-analyzer2b72a252-b817-4bc5-b7dd-e2142f6fed54\",\n",
      "  \"apiVersion\": \"2024-12-01-preview\",\n",
      "  \"createdAt\": \"2026-01-15T21:22:45Z\",\n",
      "  \"warnings\": [\n",
      "    {\n",
      "      \"code\": \"HarmfulContentDetected\",\n",
      "      \"message\": \"Content of category 'Violence' detected with severity 'Low'. Please use the field content with caution.\",\n",
      "      \"target\": \"Result.Contents[8]\"\n",
      "    }\n",
      "  ],\n",
      "  \"contents\": [\n",
      "    {\n",
      "      \"markdown\": \"# Shot 00:00.000 => 00:01.467\\n## Transcript\\n```\\nWEBVTT\\n\\n00:01.360 --> 00:06.640\\n<v Speaker>When it comes to the neural TTS, in order to get a good voice, it's better to have good data.\\n```\\n## Key Frames\\n- 00:00.733 ![](keyFrame.733.jpg)\",\n",
      "      \"fields\": {\n",
      "        \"segmentDescription\": {\n",
      "          \"type\": \"string\",\n",
      "          \"valueString\": \"A scenic aerial view of an island surrounded by the sea, showcasing the Flight Simulator and Microsoft Azure AI collaboration.\"\n",
      "        }\n",
      "      },\n",
      "      \"kind\": \"audioVisual\",\n",
      "      \"startTimeMs\": 0,\n",
      "      \"endTimeMs\": 1467,\n",
      "      \"width\": 1080,\n",
      "      \"height\": 608,\n",
      "      \"KeyFrameTimesMs\": [\n",
      "        733\n",
      "      ],\n",
      "      \"transcriptPhrases\": [\n",
      "        {\n",
      "          \"speaker\": \"speaker\",\n",
      "          \"startTimeMs\": 1360,\n",
      "          \"endTimeMs\": 6640,\n",
      "          \"text\": \"When it comes to the neural TTS, in order to get a good voice, it's better to have good data.\",\n",
      "          \"confidence\": 1,\n",
      "          \"words\": [],\n",
      "          \"locale\": \"en-US\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"markdown\": \"# Shot 00:01.467 => 00:03.233\\n## Transcript\\n```\\nWEBVTT\\n\\n00:01.360 --> 00:06.640\\n<v Speaker>When it comes to the neural TTS, in order to get a good voice, it's better to have good data.\\n```\\n## Key Frames\\n- 00:02.067 ![](keyFrame.2067.jpg)\\n- 00:02.667 ![](keyFrame.2667.jpg)\",\n",
      "      \"fields\": {\n",
      "        \"segmentDescription\": {\n",
      "          \"type\": \"string\",\n",
      "          \"valueString\": \"An indoor setting with modern design elements visible on the walls.\"\n",
      "        }\n",
      "      },\n",
      "      \"kind\": \"audioVisual\",\n",
      "      \"startTimeMs\": 1467,\n",
      "      \"endTimeMs\": 3233,\n",
      "      \"width\": 1080,\n",
      "      \"height\": 608,\n",
      "      \"KeyFrameTimesMs\": [\n",
      "        2067,\n",
      "        2667\n",
      "      ],\n",
      "      \"transcriptPhrases\": [\n",
      "        {\n",
      "          \"speaker\": \"speaker\",\n",
      "          \"startTimeMs\": 1360,\n",
      "          \"endTimeMs\": 6640,\n",
      "          \"text\": \"When it comes to the neural TTS, in order to get a good voice, it's better to have good data.\",\n",
      "          \"confidence\": 1,\n",
      "          \"words\": [],\n",
      "          \"locale\": \"en-US\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"markdown\": \"# Shot 00:03.233 => 00:07.367\\n## Transcript\\n```\\nWEBVTT\\n\\n00:01.360 --> 00:06.640\\n<v Speaker>When it comes to the neural TTS, in order to get a good voice, it's better to have good data.\\n00:07.120 --> 00:13.320\\n<v Speaker>To achieve that, we build a universal TTS model based on 3,000 hours of data.\\n```\\n## Key Frames\\n- 00:04.067 ![](keyFrame.4067.jpg)\\n- 00:04.900 ![](keyFrame.4900.jpg)\\n- 00:05.733 ![](keyFrame.5733.jpg)\\n- 00:06.567 ![](keyFrame.6567.jpg)\",\n",
      "      \"fields\": {\n",
      "        \"segmentDescription\": {\n",
      "          \"type\": \"string\",\n",
      "          \"valueString\": \"A visual representation of audio waveforms, suggesting a focus on sound or voice technology.\"\n",
      "        }\n",
      "      },\n",
      "      \"kind\": \"audioVisual\",\n",
      "      \"startTimeMs\": 3233,\n",
      "      \"endTimeMs\": 7367,\n",
      "      \"width\": 1080,\n",
      "      \"height\": 608,\n",
      "      \"KeyFrameTimesMs\": [\n",
      "        4067,\n",
      "        4900,\n",
      "        5733,\n",
      "        6567\n",
      "      ],\n",
      "      \"transcriptPhrases\": [\n",
      "        {\n",
      "          \"speaker\": \"speaker\",\n",
      "          \"startTimeMs\": 1360,\n",
      "          \"endTimeMs\": 6640,\n",
      "          \"text\": \"When it comes to the neural TTS, in order to get a good voice, it's better to have good data.\",\n",
      "          \"confidence\": 1,\n",
      "          \"words\": [],\n",
      "          \"locale\": \"en-US\"\n",
      "        },\n",
      "        {\n",
      "          \"speaker\": \"speaker\",\n",
      "          \"startTimeMs\": 7120,\n",
      "          \"endTimeMs\": 13320,\n",
      "          \"text\": \"To achieve that, we build a universal TTS model based on 3,000 hours of data.\",\n",
      "          \"confidence\": 1,\n",
      "          \"words\": [],\n",
      "          \"locale\": \"en-US\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"markdown\": \"# Shot 00:07.367 => 00:08.200\\n## Transcript\\n```\\nWEBVTT\\n\\n00:07.120 --> 00:13.320\\n<v Speaker>To achieve that, we build a universal TTS model based on 3,000 hours of data.\\n```\\n## Key Frames\\n- 00:07.800 ![](keyFrame.7800.jpg)\",\n",
      "      \"fields\": {\n",
      "        \"segmentDescription\": {\n",
      "          \"type\": \"string\",\n",
      "          \"valueString\": \"Another indoor setting, featuring sleek, modern design.\"\n",
      "        }\n",
      "      },\n",
      "      \"kind\": \"audioVisual\",\n",
      "      \"startTimeMs\": 7367,\n",
      "      \"endTimeMs\": 8200,\n",
      "      \"width\": 1080,\n",
      "      \"height\": 608,\n",
      "      \"KeyFrameTimesMs\": [\n",
      "        7800\n",
      "      ],\n",
      "      \"transcriptPhrases\": [\n",
      "        {\n",
      "          \"speaker\": \"speaker\",\n",
      "          \"startTimeMs\": 7120,\n",
      "          \"endTimeMs\": 13320,\n",
      "          \"text\": \"To achieve that, we build a universal TTS model based on 3,000 hours of data.\",\n",
      "          \"confidence\": 1,\n",
      "          \"words\": [],\n",
      "          \"locale\": \"en-US\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"markdown\": \"# Shot 00:08.200 => 00:11.367\\n## Transcript\\n```\\nWEBVTT\\n\\n00:07.120 --> 00:13.320\\n<v Speaker>To achieve that, we build a universal TTS model based on 3,000 hours of data.\\n```\\n## Key Frames\\n- 00:09.000 ![](keyFrame.9000.jpg)\\n- 00:09.800 ![](keyFrame.9800.jpg)\\n- 00:10.600 ![](keyFrame.10600.jpg)\",\n",
      "      \"fields\": {\n",
      "        \"segmentDescription\": {\n",
      "          \"type\": \"string\",\n",
      "          \"valueString\": \"An expansive view of a rural landscape with fields and structures, possibly a data center or tech facility.\"\n",
      "        }\n",
      "      },\n",
      "      \"kind\": \"audioVisual\",\n",
      "      \"startTimeMs\": 8200,\n",
      "      \"endTimeMs\": 11367,\n",
      "      \"width\": 1080,\n",
      "      \"height\": 608,\n",
      "      \"KeyFrameTimesMs\": [\n",
      "        9000,\n",
      "        9800,\n",
      "        10600\n",
      "      ],\n",
      "      \"transcriptPhrases\": [\n",
      "        {\n",
      "          \"speaker\": \"speaker\",\n",
      "          \"startTimeMs\": 7120,\n",
      "          \"endTimeMs\": 13320,\n",
      "          \"text\": \"To achieve that, we build a universal TTS model based on 3,000 hours of data.\",\n",
      "          \"confidence\": 1,\n",
      "          \"words\": [],\n",
      "          \"locale\": \"en-US\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"markdown\": \"# Shot 00:11.367 => 00:13.567\\n## Transcript\\n```\\nWEBVTT\\n\\n00:07.120 --> 00:13.320\\n<v Speaker>To achieve that, we build a universal TTS model based on 3,000 hours of data.\\n00:13.440 --> 00:23.680\\n<v Speaker>We actually accumulated tons of the data so that this universal model is able to capture the nuance of the audio and generate a more natural voice for the algorithm.\\n```\\n## Key Frames\\n- 00:12.100 ![](keyFrame.12100.jpg)\\n- 00:12.833 ![](keyFrame.12833.jpg)\",\n",
      "      \"fields\": {\n",
      "        \"segmentDescription\": {\n",
      "          \"type\": \"string\",\n",
      "          \"valueString\": \"A dimly lit hallway filled with servers, indicating a data center environment.\"\n",
      "        }\n",
      "      },\n",
      "      \"kind\": \"audioVisual\",\n",
      "      \"startTimeMs\": 11367,\n",
      "      \"endTimeMs\": 13567,\n",
      "      \"width\": 1080,\n",
      "      \"height\": 608,\n",
      "      \"KeyFrameTimesMs\": [\n",
      "        12100,\n",
      "        12833\n",
      "      ],\n",
      "      \"transcriptPhrases\": [\n",
      "        {\n",
      "          \"speaker\": \"speaker\",\n",
      "          \"startTimeMs\": 7120,\n",
      "          \"endTimeMs\": 13320,\n",
      "          \"text\": \"To achieve that, we build a universal TTS model based on 3,000 hours of data.\",\n",
      "          \"confidence\": 1,\n",
      "          \"words\": [],\n",
      "          \"locale\": \"en-US\"\n",
      "        },\n",
      "        {\n",
      "          \"speaker\": \"speaker\",\n",
      "          \"startTimeMs\": 13440,\n",
      "          \"endTimeMs\": 23680,\n",
      "          \"text\": \"We actually accumulated tons of the data so that this universal model is able to capture the nuance of the audio and generate a more natural voice for the algorithm.\",\n",
      "          \"confidence\": 1,\n",
      "          \"words\": [],\n",
      "          \"locale\": \"en-US\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"markdown\": \"# Shot 00:13.567 => 00:16.100\\n## Transcript\\n```\\nWEBVTT\\n\\n00:13.440 --> 00:23.680\\n<v Speaker>We actually accumulated tons of the data so that this universal model is able to capture the nuance of the audio and generate a more natural voice for the algorithm.\\n```\\n## Key Frames\\n- 00:14.200 ![](keyFrame.14200.jpg)\\n- 00:14.833 ![](keyFrame.14833.jpg)\\n- 00:15.467 ![](keyFrame.15467.jpg)\",\n",
      "      \"fields\": {\n",
      "        \"segmentDescription\": {\n",
      "          \"type\": \"string\",\n",
      "          \"valueString\": \"An indoor setting with contemporary design features.\"\n",
      "        }\n",
      "      },\n",
      "      \"kind\": \"audioVisual\",\n",
      "      \"startTimeMs\": 13567,\n",
      "      \"endTimeMs\": 16100,\n",
      "      \"width\": 1080,\n",
      "      \"height\": 608,\n",
      "      \"KeyFrameTimesMs\": [\n",
      "        14200,\n",
      "        14833,\n",
      "        15467\n",
      "      ],\n",
      "      \"transcriptPhrases\": [\n",
      "        {\n",
      "          \"speaker\": \"speaker\",\n",
      "          \"startTimeMs\": 13440,\n",
      "          \"endTimeMs\": 23680,\n",
      "          \"text\": \"We actually accumulated tons of the data so that this universal model is able to capture the nuance of the audio and generate a more natural voice for the algorithm.\",\n",
      "          \"confidence\": 1,\n",
      "          \"words\": [],\n",
      "          \"locale\": \"en-US\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"markdown\": \"# Shot 00:16.100 => 00:19.433\\n## Transcript\\n```\\nWEBVTT\\n\\n00:13.440 --> 00:23.680\\n<v Speaker>We actually accumulated tons of the data so that this universal model is able to capture the nuance of the audio and generate a more natural voice for the algorithm.\\n```\\n## Key Frames\\n- 00:16.933 ![](keyFrame.16933.jpg)\\n- 00:17.767 ![](keyFrame.17767.jpg)\\n- 00:18.600 ![](keyFrame.18600.jpg)\",\n",
      "      \"fields\": {\n",
      "        \"segmentDescription\": {\n",
      "          \"type\": \"string\",\n",
      "          \"valueString\": \"Aerial footage of a biplane flying over a coastal area with clear waters and urban landscapes.\"\n",
      "        }\n",
      "      },\n",
      "      \"kind\": \"audioVisual\",\n",
      "      \"startTimeMs\": 16100,\n",
      "      \"endTimeMs\": 19433,\n",
      "      \"width\": 1080,\n",
      "      \"height\": 608,\n",
      "      \"KeyFrameTimesMs\": [\n",
      "        16933,\n",
      "        17767,\n",
      "        18600\n",
      "      ],\n",
      "      \"transcriptPhrases\": [\n",
      "        {\n",
      "          \"speaker\": \"speaker\",\n",
      "          \"startTimeMs\": 13440,\n",
      "          \"endTimeMs\": 23680,\n",
      "          \"text\": \"We actually accumulated tons of the data so that this universal model is able to capture the nuance of the audio and generate a more natural voice for the algorithm.\",\n",
      "          \"confidence\": 1,\n",
      "          \"words\": [],\n",
      "          \"locale\": \"en-US\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"markdown\": \"# Shot 00:19.433 => 00:23.967\\n## Transcript\\n```\\nWEBVTT\\n\\n00:13.440 --> 00:23.680\\n<v Speaker>We actually accumulated tons of the data so that this universal model is able to capture the nuance of the audio and generate a more natural voice for the algorithm.\\n```\\n## Key Frames\\n- 00:20.167 ![](keyFrame.20167.jpg)\\n- 00:20.900 ![](keyFrame.20900.jpg)\\n- 00:21.633 ![](keyFrame.21633.jpg)\\n- 00:22.367 ![](keyFrame.22367.jpg)\\n- 00:23.100 ![](keyFrame.23100.jpg)\",\n",
      "      \"fields\": {\n",
      "        \"segmentDescription\": {\n",
      "          \"type\": \"string\",\n",
      "          \"valueString\": \"A plane flying over a picturesque castle set against a mountainous backdrop.\"\n",
      "        }\n",
      "      },\n",
      "      \"kind\": \"audioVisual\",\n",
      "      \"startTimeMs\": 19433,\n",
      "      \"endTimeMs\": 23967,\n",
      "      \"width\": 1080,\n",
      "      \"height\": 608,\n",
      "      \"KeyFrameTimesMs\": [\n",
      "        20167,\n",
      "        20900,\n",
      "        21633,\n",
      "        22367,\n",
      "        23100\n",
      "      ],\n",
      "      \"transcriptPhrases\": [\n",
      "        {\n",
      "          \"speaker\": \"speaker\",\n",
      "          \"startTimeMs\": 13440,\n",
      "          \"endTimeMs\": 23680,\n",
      "          \"text\": \"We actually accumulated tons of the data so that this universal model is able to capture the nuance of the audio and generate a more natural voice for the algorithm.\",\n",
      "          \"confidence\": 1,\n",
      "          \"words\": [],\n",
      "          \"locale\": \"en-US\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"markdown\": \"# Shot 00:23.967 => 00:30.033\\n## Transcript\\n```\\nWEBVTT\\n\\n00:24.040 --> 00:29.120\\n<v Speaker>What we liked about cognitive services offerings were that they had a much higher fidelity.\\n00:29.600 --> 00:32.880\\n<v Speaker>And they sounded a lot more like an actual human voice.\\n```\\n## Key Frames\\n- 00:24.833 ![](keyFrame.24833.jpg)\\n- 00:25.700 ![](keyFrame.25700.jpg)\\n- 00:26.567 ![](keyFrame.26567.jpg)\\n- 00:27.433 ![](keyFrame.27433.jpg)\\n- 00:28.300 ![](keyFrame.28300.jpg)\\n- 00:29.167 ![](keyFrame.29167.jpg)\",\n",
      "      \"fields\": {\n",
      "        \"segmentDescription\": {\n",
      "          \"type\": \"string\",\n",
      "          \"valueString\": \"An indoor setting with a modern office design, featuring large windows and natural light.\"\n",
      "        }\n",
      "      },\n",
      "      \"kind\": \"audioVisual\",\n",
      "      \"startTimeMs\": 23967,\n",
      "      \"endTimeMs\": 30033,\n",
      "      \"width\": 1080,\n",
      "      \"height\": 608,\n",
      "      \"KeyFrameTimesMs\": [\n",
      "        24833,\n",
      "        25700,\n",
      "        26567,\n",
      "        27433,\n",
      "        28300,\n",
      "        29167\n",
      "      ],\n",
      "      \"transcriptPhrases\": [\n",
      "        {\n",
      "          \"speaker\": \"speaker\",\n",
      "          \"startTimeMs\": 24040,\n",
      "          \"endTimeMs\": 29120,\n",
      "          \"text\": \"What we liked about cognitive services offerings were that they had a much higher fidelity.\",\n",
      "          \"confidence\": 1,\n",
      "          \"words\": [],\n",
      "          \"locale\": \"en-US\"\n",
      "        },\n",
      "        {\n",
      "          \"speaker\": \"speaker\",\n",
      "          \"startTimeMs\": 29600,\n",
      "          \"endTimeMs\": 32880,\n",
      "          \"text\": \"And they sounded a lot more like an actual human voice.\",\n",
      "          \"confidence\": 1,\n",
      "          \"words\": [],\n",
      "          \"locale\": \"en-US\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"markdown\": \"# Shot 00:30.033 => 00:33.200\\n## Transcript\\n```\\nWEBVTT\\n\\n00:29.600 --> 00:32.880\\n<v Speaker>And they sounded a lot more like an actual human voice.\\n```\\n## Key Frames\\n- 00:30.833 ![](keyFrame.30833.jpg)\\n- 00:31.633 ![](keyFrame.31633.jpg)\\n- 00:32.433 ![](keyFrame.32433.jpg)\",\n",
      "      \"fields\": {\n",
      "        \"segmentDescription\": {\n",
      "          \"type\": \"string\",\n",
      "          \"valueString\": \"A close-up view of the indoor setting, highlighting office decor.\"\n",
      "        }\n",
      "      },\n",
      "      \"kind\": \"audioVisual\",\n",
      "      \"startTimeMs\": 30033,\n",
      "      \"endTimeMs\": 33200,\n",
      "      \"width\": 1080,\n",
      "      \"height\": 608,\n",
      "      \"KeyFrameTimesMs\": [\n",
      "        30833,\n",
      "        31633,\n",
      "        32433\n",
      "      ],\n",
      "      \"transcriptPhrases\": [\n",
      "        {\n",
      "          \"speaker\": \"speaker\",\n",
      "          \"startTimeMs\": 29600,\n",
      "          \"endTimeMs\": 32880,\n",
      "          \"text\": \"And they sounded a lot more like an actual human voice.\",\n",
      "          \"confidence\": 1,\n",
      "          \"words\": [],\n",
      "          \"locale\": \"en-US\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"markdown\": \"# Shot 00:33.200 => 00:35.267\\n## Transcript\\n```\\nWEBVTT\\n\\n00:33.680 --> 00:37.200\\n<v Speaker>Orlando ground 9555 requesting the end of pushback.\\n```\\n## Key Frames\\n- 00:33.900 ![](keyFrame.33900.jpg)\\n- 00:34.600 ![](keyFrame.34600.jpg)\",\n",
      "      \"fields\": {\n",
      "        \"segmentDescription\": {\n",
      "          \"type\": \"string\",\n",
      "          \"valueString\": \"An overhead view of an airplane on the ground, with its nose directed forward.\"\n",
      "        }\n",
      "      },\n",
      "      \"kind\": \"audioVisual\",\n",
      "      \"startTimeMs\": 33200,\n",
      "      \"endTimeMs\": 35267,\n",
      "      \"width\": 1080,\n",
      "      \"height\": 608,\n",
      "      \"KeyFrameTimesMs\": [\n",
      "        33900,\n",
      "        34600\n",
      "      ],\n",
      "      \"transcriptPhrases\": [\n",
      "        {\n",
      "          \"speaker\": \"speaker\",\n",
      "          \"startTimeMs\": 33680,\n",
      "          \"endTimeMs\": 37200,\n",
      "          \"text\": \"Orlando ground 9555 requesting the end of pushback.\",\n",
      "          \"confidence\": 1,\n",
      "          \"words\": [],\n",
      "          \"locale\": \"en-US\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"markdown\": \"# Shot 00:35.267 => 00:37.700\\n## Transcript\\n```\\nWEBVTT\\n\\n00:33.680 --> 00:37.200\\n<v Speaker>Orlando ground 9555 requesting the end of pushback.\\n```\\n## Key Frames\\n- 00:36.067 ![](keyFrame.36067.jpg)\\n- 00:36.867 ![](keyFrame.36867.jpg)\",\n",
      "      \"fields\": {\n",
      "        \"segmentDescription\": {\n",
      "          \"type\": \"string\",\n",
      "          \"valueString\": \"A ground crew member signaling to an Airbus aircraft, preparing for movement.\"\n",
      "        }\n",
      "      },\n",
      "      \"kind\": \"audioVisual\",\n",
      "      \"startTimeMs\": 35267,\n",
      "      \"endTimeMs\": 37700,\n",
      "      \"width\": 1080,\n",
      "      \"height\": 608,\n",
      "      \"KeyFrameTimesMs\": [\n",
      "        36067,\n",
      "        36867\n",
      "      ],\n",
      "      \"transcriptPhrases\": [\n",
      "        {\n",
      "          \"speaker\": \"speaker\",\n",
      "          \"startTimeMs\": 33680,\n",
      "          \"endTimeMs\": 37200,\n",
      "          \"text\": \"Orlando ground 9555 requesting the end of pushback.\",\n",
      "          \"confidence\": 1,\n",
      "          \"words\": [],\n",
      "          \"locale\": \"en-US\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"markdown\": \"# Shot 00:37.700 => 00:39.200\\n## Transcript\\n```\\nWEBVTT\\n\\n00:38.680 --> 00:41.280\\n<v Speaker>9555 request to end pushback received.\\n```\\n## Key Frames\\n- 00:38.200 ![](keyFrame.38200.jpg)\\n- 00:38.700 ![](keyFrame.38700.jpg)\",\n",
      "      \"fields\": {\n",
      "        \"segmentDescription\": {\n",
      "          \"type\": \"string\",\n",
      "          \"valueString\": \"Ground crew working near aircraft on a tarmac, with the airport terminal in the background.\"\n",
      "        }\n",
      "      },\n",
      "      \"kind\": \"audioVisual\",\n",
      "      \"startTimeMs\": 37700,\n",
      "      \"endTimeMs\": 39200,\n",
      "      \"width\": 1080,\n",
      "      \"height\": 608,\n",
      "      \"KeyFrameTimesMs\": [\n",
      "        38200,\n",
      "        38700\n",
      "      ],\n",
      "      \"transcriptPhrases\": [\n",
      "        {\n",
      "          \"speaker\": \"speaker\",\n",
      "          \"startTimeMs\": 38680,\n",
      "          \"endTimeMs\": 41280,\n",
      "          \"text\": \"9555 request to end pushback received.\",\n",
      "          \"confidence\": 1,\n",
      "          \"words\": [],\n",
      "          \"locale\": \"en-US\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"markdown\": \"# Shot 00:39.200 => 00:42.033\\n## Transcript\\n```\\nWEBVTT\\n\\n00:38.680 --> 00:41.280\\n<v Speaker>9555 request to end pushback received.\\n```\\n## Key Frames\\n- 00:39.900 ![](keyFrame.39900.jpg)\\n- 00:40.600 ![](keyFrame.40600.jpg)\\n- 00:41.300 ![](keyFrame.41300.jpg)\",\n",
      "      \"fields\": {\n",
      "        \"segmentDescription\": {\n",
      "          \"type\": \"string\",\n",
      "          \"valueString\": \"An Airbus aircraft is shown at an airport gate. The lighting suggests early morning or late afternoon, with a warm glow on the plane's fuselage. This coincides with the audio indicating a request to end pushback, suggesting the aircraft is preparing for departure.\"\n",
      "        }\n",
      "      },\n",
      "      \"kind\": \"audioVisual\",\n",
      "      \"startTimeMs\": 39200,\n",
      "      \"endTimeMs\": 42033,\n",
      "      \"width\": 1080,\n",
      "      \"height\": 608,\n",
      "      \"KeyFrameTimesMs\": [\n",
      "        39900,\n",
      "        40600,\n",
      "        41300\n",
      "      ],\n",
      "      \"transcriptPhrases\": [\n",
      "        {\n",
      "          \"speaker\": \"speaker\",\n",
      "          \"startTimeMs\": 38680,\n",
      "          \"endTimeMs\": 41280,\n",
      "          \"text\": \"9555 request to end pushback received.\",\n",
      "          \"confidence\": 1,\n",
      "          \"words\": [],\n",
      "          \"locale\": \"en-US\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"markdown\": \"# Shot 00:42.033 => 00:43.866\\n## Transcript\\n```\\nWEBVTT\\n\\n```\\n## Key Frames\\n- 00:42.633 ![](keyFrame.42633.jpg)\\n- 00:43.233 ![](keyFrame.43233.jpg)\",\n",
      "      \"fields\": {\n",
      "        \"segmentDescription\": {\n",
      "          \"type\": \"string\",\n",
      "          \"valueString\": \"The screen transitions to a black background displaying the Microsoft logo, indicating a shift in focus or sponsorship.\"\n",
      "        }\n",
      "      },\n",
      "      \"kind\": \"audioVisual\",\n",
      "      \"startTimeMs\": 42033,\n",
      "      \"endTimeMs\": 43866,\n",
      "      \"width\": 1080,\n",
      "      \"height\": 608,\n",
      "      \"KeyFrameTimesMs\": [\n",
      "        42633,\n",
      "        43233\n",
      "      ],\n",
      "      \"transcriptPhrases\": []\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Iterate through each analyzer created and analyze content for each modality\n",
    "\n",
    "analyzer_results =[]\n",
    "extracted_markdown = []\n",
    "analyzer_content = []\n",
    "for analyzer in analyzer_configs:\n",
    "    analyzer_id = analyzer[\"id\"]\n",
    "    template_path = analyzer[\"template_path\"]\n",
    "    file_location = analyzer[\"location\"]\n",
    "    try:\n",
    "           # Analyze content\n",
    "            response = content_understanding_client.begin_analyze(analyzer_id, file_location)\n",
    "            result = content_understanding_client.poll_result(response)\n",
    "            analyzer_results.append({\"id\":analyzer_id, \"result\": result[\"result\"]})\n",
    "            analyzer_content.append({\"id\": analyzer_id, \"content\": result[\"result\"][\"contents\"]})\n",
    "                       \n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Error in creating analyzer. Please double-check your analysis settings.\\nIf there is a conflict, you can delete the analyzer and then recreate it, or move to the next cell and use the existing analyzer.\")\n",
    "\n",
    "print(\"Analyzer Results:\")\n",
    "for analyzer_result in analyzer_results:\n",
    "    print(f\"Analyzer ID: {analyzer_result['id']}\")\n",
    "    print(json.dumps(analyzer_result[\"result\"], indent=2))            \n",
    "\n",
    "# Delete the analyzer if it is no longer needed\n",
    "#content_understanding_client.delete_analyzer(ANALYZER_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organize multimodal data\n",
    "This is a simple starting point. Feel free to give your own chunking strategies a try!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess JSON output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     47\u001b[39m     output+=video    \n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m all_splits = \u001b[43mprocess_allJSON_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43manalyzer_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mThere are \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(all_splits)) + \u001b[33m\"\u001b[39m\u001b[33m documents.\u001b[39m\u001b[33m\"\u001b[39m) \n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Print the content of all doc splits\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mprocess_allJSON_content\u001b[39m\u001b[34m(all_content)\u001b[39m\n\u001b[32m     27\u001b[39m output+=image\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m#convert audio json object to string and append file metadata to the string\u001b[39;00m\n\u001b[32m     30\u001b[39m audio_splits = [\n\u001b[32m     31\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThis is a json string representing an audio segment with transcription for the file located in \u001b[39m\u001b[33m\"\u001b[39m+\u001b[38;5;28mstr\u001b[39m(analyzer_configs[\u001b[32m2\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mlocation\u001b[39m\u001b[33m\"\u001b[39m])+\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m \n\u001b[32m     32\u001b[39m    + v\n\u001b[32m     33\u001b[39m    + \u001b[33m\"\u001b[39m\u001b[33m```\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m    \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m convert_values_to_strings(\u001b[43mall_content\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     35\u001b[39m ]\n\u001b[32m     36\u001b[39m audio = [Document(page_content=v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m audio_splits]\n\u001b[32m     37\u001b[39m output += audio\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "def convert_values_to_strings(json_obj):\n",
    "    return [str(value) for value in json_obj]\n",
    "\n",
    "#process all content and convert to string      \n",
    "def process_allJSON_content(all_content):\n",
    "\n",
    "    # Initialize empty list to store string of all content\n",
    "    output = []\n",
    "\n",
    "    document_splits = [\n",
    "        \"This is a json string representing a document with text and metadata for the file located in \"+str(analyzer_configs[0][\"location\"])+\" \"\n",
    "        + v \n",
    "        + \"```\"\n",
    "        for v in convert_values_to_strings(all_content[0][\"content\"])\n",
    "    ]\n",
    "    docs = [Document(page_content=v) for v in document_splits]\n",
    "    output += docs\n",
    "\n",
    "    #convert image json object to string and append file metadata to the string\n",
    "    image_splits = [\n",
    "       \"This is a json string representing an image verbalization and OCR extraction for the file located in \"+str(analyzer_configs[1][\"location\"])+\" \"\n",
    "       + v\n",
    "       + \"```\"\n",
    "       for v in convert_values_to_strings(all_content[1][\"content\"])\n",
    "    ]\n",
    "    image = [Document(page_content=v) for v in image_splits]\n",
    "    output+=image\n",
    "\n",
    "    #convert audio json object to string and append file metadata to the string\n",
    "    audio_splits = [\n",
    "        \"This is a json string representing an audio segment with transcription for the file located in \"+str(analyzer_configs[2][\"location\"])+\" \" \n",
    "       + v\n",
    "       + \"```\"\n",
    "       for v in convert_values_to_strings(all_content[2][\"content\"])\n",
    "    ]\n",
    "    audio = [Document(page_content=v) for v in audio_splits]\n",
    "    output += audio\n",
    "\n",
    "    #convert video json object to string and append file metadata to the string\n",
    "    video_splits = [\n",
    "        \"The following is a json string representing a video segment with scene description and transcript for the file located in \"+str(analyzer_configs[3][\"location\"])+\" \"\n",
    "        + v\n",
    "        + \"```\"\n",
    "        for v in convert_values_to_strings(all_content[3][\"content\"])\n",
    "    ]\n",
    "    video = [Document(page_content=v) for v in video_splits]\n",
    "    output+=video    \n",
    "    \n",
    "    return output\n",
    "\n",
    "all_splits = process_allJSON_content(analyzer_content)\n",
    "\n",
    "print(\"There are \" + str(len(all_splits)) + \" documents.\") \n",
    "# Print the content of all doc splits\n",
    "for doc in all_splits:\n",
    "    print(f\"doc content\", doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Optional* - Split document markdown into semantic chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configure langchain text splitting settings\n",
    "EMBEDDING_CHUNK_SIZE = 512\n",
    "EMBEDDING_CHUNK_OVERLAP = 20\n",
    "\n",
    "# Split the document into chunks base on markdown headers.\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "text_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "docs_string = analyzer_content[0]['content'][0]['markdown'] #extract document analyzer markdown (first item in the list) is the document analyzer markdown output\n",
    "docs_splits = text_splitter.split_text(docs_string)\n",
    "\n",
    "print(\"Length of splits: \" + str(len(docs_splits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed and index the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the splitted documents and insert into Azure Search vector store\n",
    "def embed_and_index_chunks(docs):\n",
    "    aoai_embeddings = AzureOpenAIEmbeddings(\n",
    "        azure_deployment=AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME,\n",
    "        openai_api_version=AZURE_OPENAI_EMBEDDING_API_VERSION,  # e.g., \"2023-12-01-preview\"\n",
    "        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "        azure_ad_token_provider=token_provider\n",
    "    )\n",
    "\n",
    "    vector_store: AzureSearch = AzureSearch(\n",
    "        azure_search_endpoint=AZURE_SEARCH_ENDPOINT,\n",
    "        azure_search_key=None,\n",
    "        index_name=AZURE_SEARCH_INDEX_NAME,\n",
    "        embedding_function=aoai_embeddings.embed_query\n",
    "    )\n",
    "    vector_store.add_documents(documents=docs)\n",
    "    return vector_store\n",
    "\n",
    "\n",
    "# embed and index the docs:\n",
    "vector_store = embed_and_index_chunks(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve relevant chunks based on a question\n",
    "#### Execute a pure vector similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your query\n",
    "query = \"japan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a similarity search\n",
    "docs = vector_store.similarity_search(\n",
    "    query=query,\n",
    "    k=3,\n",
    "    search_type=\"similarity\",\n",
    ")\n",
    "for doc in docs:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute hybrid search. Vector and nonvector text fields are queried in parallel, results are merged, and top matches of the unified result set are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a hybrid search using the search_type parameter\n",
    "docs = vector_store.hybrid_search(query=query, k=3)\n",
    "for doc in docs:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A\n",
    "We can utilize OpenAI GPT completion models + Azure Search to conversationally search for and chat about the results. (If you are using GitHub Codespaces, there will be an input prompt near the top of the screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup rag chain\n",
    "prompt_str = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\"\"\"\n",
    "\n",
    "\n",
    "def setup_rag_chain(vector_store):\n",
    "    retriever = vector_store.as_retriever(search_type=\"similarity\", k=3)\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_str)\n",
    "    llm = AzureChatOpenAI(\n",
    "        openai_api_version=AZURE_OPENAI_CHAT_API_VERSION,\n",
    "        azure_deployment=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "        azure_ad_token_provider=token_provider,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return rag_chain\n",
    "\n",
    "\n",
    "# Setup conversational search\n",
    "def conversational_search(rag_chain, query):\n",
    "    print(rag_chain.invoke(query))\n",
    "\n",
    "\n",
    "rag_chain = setup_rag_chain(vector_store)\n",
    "while True:\n",
    "    query = input(\"Enter your query: \")\n",
    "    if query==\"\":\n",
    "        break\n",
    "    conversational_search(rag_chain, query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
